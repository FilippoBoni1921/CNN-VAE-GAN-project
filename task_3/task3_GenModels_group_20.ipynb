{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GC-hsi4hJhUu"
   },
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oJQlRV9BNW1U"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "giqOrGLWJcm1"
   },
   "outputs": [],
   "source": [
    "def load_real_samples(scale=False):\n",
    "    # We load 20,000 samples only to avoid memory issues, you can  change this value\n",
    "    X = np.load(\"cat_img.npy\")\n",
    "    # Scale samples in range [-127, 127]\n",
    "    if scale:\n",
    "        X = (X - 127.5) * 2\n",
    "    return X / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hnsWPq-GJcpz"
   },
   "outputs": [],
   "source": [
    "dataset = load_real_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1vvpr5dhJcsk"
   },
   "outputs": [],
   "source": [
    "def grid_plot(images, epoch='', name='', n=3, save=False, scale=False):\n",
    "    if scale:\n",
    "        images = (images + 1) / 2.0\n",
    "    for index in range(n * n):\n",
    "        plt.subplot(n, n, 1 + index)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(images[index])\n",
    "    fig = plt.gcf()\n",
    "    fig.suptitle(name + '  '+ str(epoch), fontsize=14)\n",
    "    if save:\n",
    "        filename = '/results/'+name+'.png' \n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "grid_plot(dataset[np.random.randint(0, 1000, 9)], name='Fliqr dataset (64x64x3)', n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5RgdTAI5JcvS"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Conv2DTranspose, Reshape\n",
    "\n",
    "def build_conv_net(in_shape, out_shape, n_downsampling_layers=4, filters=128, out_activation='sigmoid'):\n",
    "    \"\"\"\n",
    "    Build a basic convolutional network\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential()\n",
    "    default_args=dict(kernel_size=(3,3), strides=(2,2), padding='same', activation='relu')\n",
    "\n",
    "    model.add(Conv2D(input_shape=in_shape, **default_args, filters=filters))\n",
    "\n",
    "    for _ in range(n_downsampling_layers):\n",
    "        model.add(Conv2D(**default_args, filters=filters))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(out_shape, activation=out_activation) )\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_deconv_net(latent_dim, n_upsampling_layers=4, filters=128, activation_out='sigmoid'):\n",
    "    \"\"\"\n",
    "    Build a deconvolutional network for decoding/upscaling latent vectors\n",
    "\n",
    "    When building the deconvolutional architecture, usually it is best to use the same layer sizes that \n",
    "    were used in the downsampling network and the Conv2DTranspose layers are used instead of Conv2D layers. \n",
    "    Using identical layers and hyperparameters ensures that the dimensionality of our output matches the\n",
    "    shape of our input images. \n",
    "    \"\"\"\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Dense(4 * 4 * 64, input_dim=latent_dim)) \n",
    "    model.add(Reshape((4, 4, 64))) # This matches the output size of the downsampling architecture\n",
    "    default_args=dict(kernel_size=(3,3), strides=(2,2), padding='same', activation='relu')\n",
    "    \n",
    "    for i in range(n_upsampling_layers):\n",
    "        model.add(Conv2DTranspose(**default_args, filters=filters))\n",
    "\n",
    "    # This last convolutional layer converts back to 3 channel RGB image\n",
    "    model.add(Conv2D(filters=3, kernel_size=(3,3), activation=activation_out, padding='same'))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rXD47myIJcx2"
   },
   "outputs": [],
   "source": [
    "class Sampling(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Custom layer for the variational autoencoder\n",
    "    It takes two vectors as input - one for means and other for variances of the latent variables described by a multimodal gaussian\n",
    "    Its output is a latent vector randomly sampled from this distribution\n",
    "    \"\"\"\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_var) * epsilon\n",
    "\n",
    "def build_vae(data_shape, latent_dim, filters=128):\n",
    "\n",
    "    # Building the encoder - starts with a simple downsampling convolutional network  \n",
    "    encoder = build_conv_net(data_shape, latent_dim*2, filters=filters)\n",
    "    \n",
    "    # Adding special sampling layer that uses the reparametrization trick \n",
    "    z_mean = Dense(latent_dim)(encoder.output)\n",
    "    z_var = Dense(latent_dim)(encoder.output)\n",
    "    z = Sampling()([z_mean, z_var])\n",
    "    \n",
    "    # Connecting the two encoder parts\n",
    "    encoder = tf.keras.Model(inputs=encoder.input, outputs=z)\n",
    "\n",
    "    # Defining the decoder which is a regular upsampling deconvolutional network\n",
    "    decoder = build_deconv_net(latent_dim, activation_out='sigmoid', filters=filters)\n",
    "    vae = tf.keras.Model(inputs=encoder.input, outputs=decoder(z))\n",
    "    \n",
    "    # Adding the special loss term\n",
    "    kl_loss = -0.5 * tf.reduce_sum(z_var - tf.square(z_mean) - tf.exp(z_var) + 1)\n",
    "    vae.add_loss(kl_loss/tf.cast(tf.keras.backend.prod(data_shape), tf.float32))\n",
    "\n",
    "    vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss='binary_crossentropy')\n",
    "\n",
    "    return encoder, decoder, vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BwnD3TJPJc0S"
   },
   "outputs": [],
   "source": [
    "latent_dim = 32\n",
    "encoder, decoder, vae = build_vae(dataset.shape[1:], latent_dim, filters=128)\n",
    "loss = []\n",
    "# Generate random vectors that we will use to sample our latent space\n",
    "for epoch in range(50):\n",
    "    latent_vectors = np.random.randn(9, latent_dim)\n",
    "    history=vae.fit(x=dataset, y=dataset, epochs=1, batch_size=8)\n",
    "    loss.append(history)\n",
    "    images = decoder(latent_vectors)\n",
    "    grid_plot(images, epoch, name='VAE generated images (randomly sampled from the latent space)', n=3, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xPyLw_SqJtsZ"
   },
   "outputs": [],
   "source": [
    "latent_dim = 32\n",
    "noise = np.random.randn(1, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TycC48WdJtvO"
   },
   "outputs": [],
   "source": [
    "vec_1 = np.random.randn(1, latent_dim)\n",
    "vec_2 = np.random.randn(1, latent_dim)\n",
    "\n",
    "def interpolate_points(p1, p2, n_steps=10):\n",
    "\t# interpolate ratios between the points\n",
    "  ratios = np.linspace(0, 1, num=n_steps)\n",
    "\t# linear interpolate vectors\n",
    "  print(latent_dim)\n",
    "  vectors = np.zeros((n_steps,latent_dim))\n",
    "  for i in range(len(ratios)):\n",
    "    vectors[i] = (1.0 - ratios[i]) * p1 + ratios[i] * p2\n",
    "  return vectors\n",
    " \n",
    "def plot_generated(examples, n, save = True):\n",
    "\t# plot images\n",
    "\tfor i in range(n):\n",
    "\t\tplt.subplot(1, n, 1 + i)\n",
    "\t\tplt.axis('off')\n",
    "\t\tplt.imshow(examples[i, :, :])\n",
    "\n",
    "def plot_generated(examples, n):\n",
    "  for i in range(n):\n",
    "    plt.subplot(1, n, 1 + i)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(examples[i, :, :])\n",
    "\n",
    "  plt.gcf().set_size_inches(45, 15)\n",
    "  plt.savefig('results/VAE_lat.png')\n",
    "  \n",
    "\n",
    "interpolated = interpolate_points(vec_1, vec_2)\n",
    "print(np.shape(interpolated))\n",
    "\n",
    "images = decoder.predict(interpolated)\n",
    "#images = (images + 1) / 2.0\n",
    "plot_generated(images, len(interpolated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9Jr8b-gOl-Y"
   },
   "source": [
    "# GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_MdoKLsnJc20"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Ri0A8iiOqQW"
   },
   "outputs": [],
   "source": [
    "dataset = np.load(\"cat_img.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LRQdJIydOqTP"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "img = plt.imshow(dataset[0])\n",
    "img.set_cmap('hot')\n",
    "plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o05-M798OqVz"
   },
   "outputs": [],
   "source": [
    "def load_real_samples(dataset,scale=False):\n",
    "    # We load 20,000 samples only to avoid memory issues, you can  change this value\n",
    "    X = dataset\n",
    "    # Scale samples in range [-127, 127]\n",
    "    if scale:\n",
    "        X = (X - 127.5) * 2\n",
    "    return X / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LOzv6BVUOwb8"
   },
   "outputs": [],
   "source": [
    "def grid_plot(images, epoch='', name='', n=3, save=False, scale=False):\n",
    "    if scale:\n",
    "        images = (images + 1) / 2.0\n",
    "    \n",
    "    for index in range(n * n):\n",
    "\n",
    "        plt.subplot(n, n, 1 + index)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(images[index])\n",
    "    fig = plt.gcf()\n",
    "    fig.suptitle(name + '  ', fontsize=14)\n",
    "    if save:\n",
    "        filename = '/results/'+ name +'.png'\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "    plt.show()\n",
    "\n",
    "grid_plot(dataset[np.random.randint(0, 1000, 9)], name='Fliqr dataset (64x64x3)', n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4OACTmmiJc5Y"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Conv2DTranspose, Reshape, BatchNormalization, LeakyReLU, GaussianNoise\n",
    "\n",
    "def build_conv_net(in_shape, out_shape, n_downsampling_layers=4, filters=128, out_activation='sigmoid'):\n",
    "    \"\"\"\n",
    "    Build a basic convolutional network\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential()\n",
    "    default_args=dict(kernel_size=(3,3), strides=(2,2), padding='same')\n",
    "\n",
    "    model.add(Conv2D(input_shape=in_shape, kernel_size=(3,3), strides=(2,2), padding='same', filters=filters))\n",
    "    model.add(LeakyReLU())\n",
    "    \n",
    "\n",
    "    for _ in range(n_downsampling_layers):\n",
    "        model.add(Conv2D(**default_args, filters=filters))\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(GaussianNoise(0.01))\n",
    "        \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(out_shape, activation='linear') )\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_deconv_net(latent_dim, n_upsampling_layers=4, filters=128, activation_out='sigmoid'):\n",
    "    \"\"\"\n",
    "    Build a deconvolutional network for decoding/upscaling latent vectors\n",
    "\n",
    "    When building the deconvolutional architecture, usually it is best to use the same layer sizes that \n",
    "    were used in the downsampling network and the Conv2DTranspose layers are used instead of Conv2D layers. \n",
    "    Using identical layers and hyperparameters ensures that the dimensionality of our output matches the\n",
    "    shape of our input images. \n",
    "    \"\"\"\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Dense(4 * 4 * 64, input_dim=latent_dim)) \n",
    "    model.add(Reshape((4, 4, 64))) # This matches the output size of the downsampling architecture\n",
    "    default_args=dict(kernel_size=(3,3), strides=(2,2), padding='same', activation='relu')\n",
    "\n",
    "    \n",
    "    for i in range(n_upsampling_layers):\n",
    "        model.add(Conv2DTranspose(**default_args, filters=filters))\n",
    "       \n",
    "    # This last convolutional layer converts back to 3 channel RGB image\n",
    "    model.add(Conv2D(filters=3, kernel_size=(3,3), activation=activation_out, padding='same'))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dJGkRGYFO3YC"
   },
   "outputs": [],
   "source": [
    "def build_gan(data_shape, latent_dim, filters=128, lr=0.0002, beta_1=0.5):\n",
    "    optimizer = tf.optimizers.Adam(learning_rate=lr, beta_1=beta_1)\n",
    "\n",
    "    # Usually thew GAN generator has tanh activation function in the output layer\n",
    "    generator = build_deconv_net(latent_dim, activation_out='tanh', filters=filters)\n",
    "    \n",
    "    # Build and compile the discriminator\n",
    "    discriminator = build_conv_net(in_shape=data_shape, out_shape=1, filters=filters) # Single output for binary classification\n",
    "    discriminator.compile(loss='mse', optimizer=optimizer, metrics = ['accuracy'])\n",
    "    \n",
    "    # End-to-end GAN model for training the generator\n",
    "    discriminator.trainable = False\n",
    "    true_fake_prediction = discriminator(generator.output)\n",
    "    GAN = tf.keras.Model(inputs=generator.input, outputs=true_fake_prediction)\n",
    "    GAN = tf.keras.models.Sequential([generator, discriminator])\n",
    "    GAN.compile(loss='mse', optimizer=optimizer)\n",
    "    \n",
    "    return discriminator, generator, GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VqJyDepmO57G"
   },
   "outputs": [],
   "source": [
    "def run_generator(generator, n_samples=100):\n",
    "    \"\"\"\n",
    "    Run the generator model and generate n samples of synthetic images using random latent vectors\n",
    "    \"\"\"\n",
    "    latent_dim = generator.layers[0].input_shape[-1]\n",
    "    generator_input = np.random.randn(n_samples, latent_dim)\n",
    "\n",
    "    return generator.predict(generator_input)\n",
    "    \n",
    "\n",
    "def get_batch(generator, dataset, batch_size=32):\n",
    "    \"\"\"\n",
    "    Gets a single batch of samples (X) and labels (y) for the training the discriminator.\n",
    "    One half from the real dataset (labeled as 1s), the other created by the generator model (labeled as 0s).\n",
    "    \"\"\"\n",
    "    batch_size //= 2 # Split evenly among fake and real samples\n",
    "\n",
    "    fake_data = run_generator(generator, n_samples=batch_size)\n",
    "    real_data = dataset[np.random.randint(0, dataset.shape[0], batch_size)]\n",
    "\n",
    "    X = np.concatenate([fake_data, real_data], axis=0)\n",
    "    y = np.concatenate([np.zeros([batch_size, 1]), np.ones([batch_size, 1])], axis=0)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def train_gan(generator, discriminator, gan, dataset, latent_dim, n_epochs=20, batch_size=32):\n",
    "\n",
    "    batches_per_epoch = int(dataset.shape[0] / batch_size / 2)\n",
    "\n",
    "    acc = np.zeros(shape=(n_epochs,batches_per_epoch))\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        i = 0\n",
    "        for batch in tqdm(range(batches_per_epoch)):\n",
    "            \n",
    "            # 1) Train discriminator both on real and synthesized images\n",
    "            X, y = get_batch(generator, dataset, batch_size=batch_size)\n",
    "            discriminator_loss, discriminator_acc = discriminator.train_on_batch(X, y)\n",
    "\n",
    "            acc[epoch][i] = discriminator_acc\n",
    "\n",
    "            # 2) Train generator (note that now the label of synthetic images is reversed to 1)\n",
    "            X_gan = np.random.randn(batch_size, latent_dim)\n",
    "            y_gan = np.ones([batch_size, 1])\n",
    "            generator_loss = gan.train_on_batch(X_gan, y_gan)\n",
    "\n",
    "            i = i+1\n",
    "\n",
    "        noise = np.random.randn(16, latent_dim)\n",
    "        images = generator.predict(noise)\n",
    "        grid_plot(images, epoch, name='GAN generated images', n=3, save=False, scale=True)\n",
    "   \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUDBPVSGO6Ca"
   },
   "outputs": [],
   "source": [
    "latent_dim = 256\n",
    "discriminator, generator, gan = build_gan(dataset.shape[1:], latent_dim, filters=128)\n",
    "dataset_scaled = load_real_samples(dataset,scale=True)\n",
    "\n",
    "acc = train_gan(generator, discriminator, gan, dataset_scaled, latent_dim, n_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "latent_dim = 256\n",
    "noise = np.random.randn(1, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KKARO0j6O9lT"
   },
   "outputs": [],
   "source": [
    "images = generator.predict(noise)\n",
    "grid_plot(images, 0, name='GAN generated images', n=1, save=False, scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IwaR70l4O9oH"
   },
   "outputs": [],
   "source": [
    "noise = np.random.randn(1, 256)\n",
    "latent_dim=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jPJJL1PGO9q-"
   },
   "outputs": [],
   "source": [
    "vec_1 = np.random.randn(1, latent_dim)\n",
    "vec_2 = np.random.randn(1, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ly3lsUtUPJR8"
   },
   "outputs": [],
   "source": [
    "def interpolate_points(p1, p2, n_steps=10):\n",
    "\t# interpolate ratios between the points\n",
    "  ratios = np.linspace(0, 1, num=n_steps)\n",
    "\t# linear interpolate vectors\n",
    "  vectors = np.zeros((n_steps,256))\n",
    "  for i in range(len(ratios)):\n",
    "    vectors[i] = (1.0 - ratios[i]) * p1 + ratios[i] * p2\n",
    "  return vectors\n",
    " \n",
    "def plot_generated(examples, n, save = True):\n",
    "\t# plot images\n",
    "\tfor i in range(n):\n",
    "\t\tplt.subplot(1, n, 1 + i)\n",
    "\t\tplt.axis('off')\n",
    "\t\tplt.imshow(examples[i, :, :])\n",
    "\n",
    "def plot_generated(examples, n):\n",
    "  for i in range(n):\n",
    "    plt.subplot(1, n, 1 + i)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(examples[i, :, :])\n",
    "\n",
    "  plt.gcf().set_size_inches(45, 15)\n",
    "  plt.savefig('results/GAN_lat.png')\n",
    "  #plt.close()\n",
    "  \n",
    "\n",
    "interpolated = interpolate_points(vec_1, vec_2)\n",
    "print(np.shape(interpolated))\n",
    "\n",
    "images = generator.predict(interpolated)\n",
    "images = (images + 1) / 2.0\n",
    "plot_generated(images, len(interpolated))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd11e0d5da048d47cfc219ec71ffed3a9ca3b61b7bae9d86177cac6d2bbb3502"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
